# -*- coding: utf-8 -*-
"""AI_Streamlit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1he2iCVVHqGEX39Ey7uQdWVXInGfDzme7
"""

#!pip install streamlit openai-whisper ffmpeg-python
#!apt-get install -y ffmpeg

from flask import Flask, render_template_string, request, jsonify
import whisper
import uuid
import os
import ffmpeg

app = Flask(__name__)
UPLOAD_FOLDER = "uploads"
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

# Load Whisper model
model = whisper.load_model("base")

# Define interview questions
questions = [
    "Tell me about yourself.",
    "What are your strengths?",
    "Describe a technical challenge you've faced.",
    "Why do you want this job?",
    "Where do you see yourself in five years?"
]

@app.route("/")
def index():
    # JavaScript to speak each question with delays
    js_questions = "\n".join([
        f"setTimeout(() => speak('{q}'), {i * 8000});"
        for i, q in enumerate(questions)
    ])
    total_time = len(questions) * 8 + 2

    return render_template_string("""
    <!DOCTYPE html>
    <html>
    <head>
        <title>AI Interview Bot</title>
    </head>
    <body>
      <h2>üé§ AI Interview</h2>
      <video id="preview" autoplay muted playsinline></video><br><br>
      <button onclick="startInterview()">‚ñ∂Ô∏è Start Interview</button>

      <script>
        const constraints = { audio: true, video: true };
        let mediaRecorder;
        let recordedChunks = [];

        function speak(text) {
          const msg = new SpeechSynthesisUtterance(text);
          window.speechSynthesis.speak(msg);
        }

        async function startInterview() {
          const stream = await navigator.mediaDevices.getUserMedia(constraints);
          document.getElementById('preview').srcObject = stream;

          mediaRecorder = new MediaRecorder(stream);
          recordedChunks = [];

          mediaRecorder.ondataavailable = (event) => {
            if (event.data.size > 0) {
              recordedChunks.push(event.data);
            }
          };

          mediaRecorder.onstop = () => {
            const blob = new Blob(recordedChunks, { type: 'video/webm' });
            const formData = new FormData();
            formData.append('video', blob, 'interview.webm');

            fetch('/upload', {
              method: 'POST',
              body: formData
            })
            .then(response => response.json())
            .then(data => {
              alert("‚úÖ Interview uploaded! Visit /transcript/" + data.filename + " to view the transcript.");
            });
          };

          mediaRecorder.start();

          // Speak questions with delay
          {{ js_questions | safe }}

          // Stop recording after total duration
          setTimeout(() => {
            mediaRecorder.stop();
          }, {{ total_time * 1000 }});
        }
      </script>
    </body>
    </html>
    """, js_questions=js_questions, total_time=total_time)


@app.route("/upload", methods=["POST"])
def upload_video():
    video = request.files['video']
    filename = f"{uuid.uuid4().hex}.webm"
    filepath = os.path.join(UPLOAD_FOLDER, filename)
    video.save(filepath)
    return jsonify({"filename": filename})


@app.route("/transcript/<filename>")
def get_transcript(filename):
    webm_path = os.path.join(UPLOAD_FOLDER, filename)
    wav_path = webm_path.replace(".webm", ".wav")

    ffmpeg.input(webm_path).output(wav_path).run(overwrite_output=True)

    result = model.transcribe(wav_path)
    transcript = result["text"]

    return render_template_string("""
    <h2>üìù Transcript</h2>
    <p><b>Questions Asked:</b></p>
    <ol>
      {% for q in questions %}
      <li>{{ q }}</li>
      {% endfor %}
    </ol>
    <hr>
    <h3>üéß Full Transcript:</h3>
    <p>{{ transcript }}</p>
    """, questions=questions, transcript=transcript)


if __name__ == "__main__":
    app.run(debug=True)

