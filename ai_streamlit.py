# -*- coding: utf-8 -*-
"""AI_Streamlit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1he2iCVVHqGEX39Ey7uQdWVXInGfDzme7
"""

#!pip install streamlit openai-whisper ffmpeg-python
#!apt-get install -y ffmpeg

import streamlit as st
import streamlit.components.v1 as components
import whisper
import ffmpeg
import uuid
import os

# Load Whisper model (cached for speed)
@st.cache_resource
def load_model():
    return whisper.load_model("base")  # You can also try "tiny" or "small"

model = load_model()

# Interview Questions
questions = [
    "Tell me about yourself.",
    "What are your strengths?",
    "Describe a technical challenge you've faced.",
    "Why do you want this job?",
    "Where do you see yourself in five years?"
]

# Streamlit App UI
st.set_page_config(page_title="ğŸ¥ AI Interview Bot", layout="centered")
st.title("ğŸ¥ AI Interview Bot (Click to Start)")

st.markdown("""
Click the button below to start your video interview.  
Your **camera and microphone** will activate.  
The bot will ask you **5 questions**, one by one using text-to-speech.  
Once done, your **recording will download automatically**, and you can **upload it here** for transcript analysis.
""")

# Generate unique video file name
video_filename = f"interview_{uuid.uuid4().hex}.webm"

# Button to start interview
if st.button("â–¶ï¸ Start Interview"):
    # JavaScript for bot TTS question timing
    js_questions = "\n".join([
        f"setTimeout(() => speak('{q}'), {i * 8000});"
        for i, q in enumerate(questions)
    ])
    total_time = len(questions) * 8 + 2  # seconds

    # Inject HTML + JS for webcam, mic, MediaRecorder, TTS
    components.html(f"""
    <!DOCTYPE html>
    <html>
    <body>
      <h3>ğŸ¤ Interview in Progress...</h3>
      <video id="preview" autoplay muted playsinline></video>
      <script>
        const constraints = {{ audio: true, video: true }};
        let mediaRecorder;
        let recordedChunks = [];

        function speak(text) {{
          const msg = new SpeechSynthesisUtterance(text);
          window.speechSynthesis.speak(msg);
        }}

        async function startRecording() {{
          const stream = await navigator.mediaDevices.getUserMedia(constraints);
          document.getElementById('preview').srcObject = stream;

          mediaRecorder = new MediaRecorder(stream);
          mediaRecorder.ondataavailable = e => recordedChunks.push(e.data);
          mediaRecorder.onstop = saveRecording;

          mediaRecorder.start();
          {js_questions}
          setTimeout(() => mediaRecorder.stop(), {total_time * 1000});
        }}

        function saveRecording() {{
          const blob = new Blob(recordedChunks, {{ type: 'video/webm' }});
          const a = document.createElement('a');
          a.href = URL.createObjectURL(blob);
          a.download = '{video_filename}';
          a.click();
          alert("âœ… Interview complete. Your video was downloaded. Please upload it below for analysis.");
        }}

        document.addEventListener("DOMContentLoaded", () => {{
          startRecording();
        }});
      </script>
    </body>
    </html>
    """, height=300)

# Upload the downloaded .webm interview video
uploaded_file = st.file_uploader("ğŸ“¤ Upload the downloaded .webm video file", type="webm")

# Process and transcribe
if uploaded_file and st.button("ğŸ“ Transcribe Interview"):
    # Save uploaded video
    with open(video_filename, "wb") as f:
        f.write(uploaded_file.read())

    audio_path = video_filename.replace(".webm", ".wav")
    ffmpeg.input(video_filename).output(audio_path).run(overwrite_output=True)

    # Transcribe using Whisper
    result = model.transcribe(audio_path)
    transcript = result["text"]

    st.success("âœ… Interview Transcript")
    for i, q in enumerate(questions):
        st.markdown(f"**Q{i+1}: {q}**")
    st.markdown("---")
    st.markdown("**ğŸ§¾ Full Transcript of Your Answer:**")
    st.write(transcript)
