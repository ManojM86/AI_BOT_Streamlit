# -*- coding: utf-8 -*-
"""AI_Streamlit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1he2iCVVHqGEX39Ey7uQdWVXInGfDzme7
"""

!pip install streamlit openai-whisper ffmpeg-python
!apt-get install -y ffmpeg

import streamlit as st
import streamlit.components.v1 as components
import whisper
import ffmpeg
import uuid
import os
import time

# Whisper model
model = whisper.load_model("base")

# Questions
questions = [
    "Tell me about yourself.",
    "What are your strengths?",
    "Describe a technical challenge you've faced.",
    "Why do you want this job?",
    "Where do you see yourself in five years?"
]

st.set_page_config(page_title="ğŸ¥ AI Interview Bot", layout="centered")
st.title("ğŸ¥ AI Interview Bot (Automated Recording)")

st.markdown("""
Once the page loads, your camera and microphone will start recording automatically.
The bot will ask you interview questions one by one.
After all questions are asked, the recording will stop, and your response will be transcribed.
""")

video_filename = f"interview_{uuid.uuid4().hex}.webm"

# Create JavaScript to speak questions at intervals
js_questions = "\n".join([f"setTimeout(() => speak('{q}'), {i*8000});" for i, q in enumerate(questions)])
total_time = len(questions) * 8 + 2

components.html(f"""
<!DOCTYPE html>
<html>
<body>
  <h3>ğŸ¤ Recording in progress...</h3>
  <video id="preview" autoplay muted playsinline></video>
  <script>
    const constraints = {{ audio: true, video: true }};
    let mediaRecorder;
    let recordedChunks = [];

    function speak(text) {{
      const msg = new SpeechSynthesisUtterance(text);
      window.speechSynthesis.speak(msg);
    }}

    async function startRecording() {{
      const stream = await navigator.mediaDevices.getUserMedia(constraints);
      document.getElementById('preview').srcObject = stream;
      mediaRecorder = new MediaRecorder(stream);
      mediaRecorder.ondataavailable = e => recordedChunks.push(e.data);
      mediaRecorder.onstop = saveRecording;

      mediaRecorder.start();
      {js_questions}
      setTimeout(() => mediaRecorder.stop(), {total_time * 1000});
    }}

    function saveRecording() {{
      const blob = new Blob(recordedChunks, {{ type: 'video/webm' }});
      const a = document.createElement('a');
      a.href = URL.createObjectURL(blob);
      a.download = '{video_filename}';
      a.click();
      alert("âœ… Interview complete. Video will be downloaded.");
    }}

    startRecording();
  </script>
</body>
</html>
""", height=300)

# Manual upload for transcription
uploaded_file = st.file_uploader("ğŸ“¤ Upload the recorded .webm file to transcribe", type="webm")
if uploaded_file and st.button("ğŸ“ Transcribe Interview"):
    # Save uploaded file
    with open(video_filename, "wb") as f:
        f.write(uploaded_file.read())

    audio_path = video_filename.replace(".webm", ".wav")
    ffmpeg.input(video_filename).output(audio_path).run(overwrite_output=True)

    result = model.transcribe(audio_path)
    transcript = result["text"]

    st.success("âœ… Interview Transcript")
    for i, q in enumerate(questions):
        st.markdown(f"**Q{i+1}: {q}**")
    st.markdown("---")
    st.markdown("**ğŸ§¾ Transcript of your answer:**")
    st.write(transcript)

