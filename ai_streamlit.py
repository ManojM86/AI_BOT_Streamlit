# -*- coding: utf-8 -*-
"""AI_Streamlit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1he2iCVVHqGEX39Ey7uQdWVXInGfDzme7
"""

#!pip install streamlit openai-whisper ffmpeg-python
#!apt-get install -y ffmpeg

import streamlit as st
import streamlit.components.v1 as components
import whisper
import ffmpeg
import uuid
import os

# Load Whisper model (cached for performance)
@st.cache_resource
def load_model():
    return whisper.load_model("base")  # use "tiny" or "small" to make it faster

model = load_model()

# Define Interview Questions
questions = [
    "Tell me about yourself.",
    "What are your strengths?",
    "Describe a technical challenge you've faced.",
    "Why do you want this job?",
    "Where do you see yourself in five years?"
]

# Streamlit App Layout
st.set_page_config(page_title="üé• AI Interview Bot", layout="centered")
st.title("üé• AI Interview Bot")

st.markdown("""
Click the button inside the video area to start your video interview.

The bot will ask **5 questions** using voice.

Your **video and audio** will be recorded and **downloaded automatically**.

Then, upload the video below to generate the **transcript**.
""")

video_filename = f"interview_{uuid.uuid4().hex}.webm"

# JS timing to speak questions at intervals
js_questions = "\n".join([
    f"setTimeout(() => speak('{q}'), {i * 8000});"
    for i, q in enumerate(questions)
])
total_time = len(questions) * 8 + 2  # Total duration in seconds

# Embed HTML + JS inside iframe
components.html(f"""
<!DOCTYPE html>
<html>
  <body>
    <h3>üé§ Interview Ready</h3>
    <video id="preview" autoplay muted playsinline></video><br>
    <button onclick="startInterview()">‚ñ∂Ô∏è Start Interview</button>
    <script>
      const constraints = {{ audio: true, video: true }};
      let mediaRecorder;
      let recordedChunks = [];

      function speak(text) {{
        const msg = new SpeechSynthesisUtterance(text);
        window.speechSynthesis.speak(msg);
      }}

      async function startInterview() {{
        const stream = await navigator.mediaDevices.getUserMedia(constraints);
        document.getElementById('preview').srcObject = stream;

        mediaRecorder = new MediaRecorder(stream);
        mediaRecorder.ondataavailable = e => recordedChunks.push(e.data);
        mediaRecorder.onstop = saveRecording;
        mediaRecorder.start();

        // Ask questions with delay
        {js_questions}
        setTimeout(() => mediaRecorder.stop(), {total_time * 1000});
      }}

      function saveRecording() {{
        const blob = new Blob(recordedChunks, {{ type: 'video/webm' }});
        const a = document.createElement('a');
        a.href = URL.createObjectURL(blob);
        a.download = '{video_filename}';
        a.click();
        alert("‚úÖ Interview complete. Your video has been downloaded. Please upload it below for transcription.");
      }}
    </script>
  </body>
</html>
""", height=400)

# Upload recorded video for transcription
uploaded_file = st.file_uploader("üì§ Upload your downloaded interview video (.webm)", type="webm")

if uploaded_file and st.button("üìù Transcribe Interview"):
    # Save uploaded file
    with open(video_filename, "wb") as f:
        f.write(uploaded_file.read())

    # Convert to WAV using ffmpeg
    audio_path = video_filename.replace(".webm", ".wav")
    ffmpeg.input(video_filename).output(audio_path).run(overwrite_output=True)

    # Transcribe with Whisper
    result = model.transcribe(audio_path)
    transcript = result["text"]

    # Show Results
    st.success("‚úÖ Interview Transcript")
    for i, q in enumerate(questions):
        st.markdown(f"**Q{i+1}: {q}**")
    st.markdown("---")
    st.markdown("**üßæ Full Transcript of Your Response:**")
    st.write(transcript)
